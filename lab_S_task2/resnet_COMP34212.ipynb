{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP34212 Summative Lab Task2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Check if GPU is available\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "folder_name = 'checkpoint'\n",
    "if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "# TODO: change variable names\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic block for ResNet18 and ResNet34\n",
    "\n",
    "    Args:\n",
    "    inplanes: int, number of input channels\n",
    "    planes: int, number of output channels\n",
    "    stride: int, stride of the first convolutional layer\n",
    "    downsample: nn.Module, downsample layer\n",
    "    groups: int, number of groups for the convolutional layers\n",
    "    base_width: int, base width of the convolutional layers\n",
    "    dilation: int, dilation rate of the convolutional layers\n",
    "    norm_layer: nn.Module, normalization layer\n",
    "\n",
    "    Returns:\n",
    "    out: tensor, output of the basic block\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    For deeper models, we use the \"bottleneck\" building block to reduce the number of parameters.\n",
    "    It is based on the following principle:\n",
    "    1. A 1x1 convolution reduces the dimensionality of the input to a bottleneck representation.\n",
    "    2. A 3x3 convolution is applied to the bottleneck representation.\n",
    "    3. A 1x1 convolution increases the dimensionality of the representation back to the original.\n",
    "    4. The output is added to the original input.\n",
    "    5. The result is passed through a ReLU activation function.\n",
    "\n",
    "    The bottleneck block is used in the ResNet-50, ResNet-101, and ResNet-152 architectures.\n",
    "    \"\"\"\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The ResNet architecture is based on the following principles:\n",
    "    1. The input is passed through a 7x7 convolutional layer with stride 2.\n",
    "    2. The output is passed through a 3x3 max pooling layer with stride 2.\n",
    "    3. The output is passed through a series of residual blocks.\n",
    "    4. The output is passed through a global average pooling layer.\n",
    "    5. The output is passed through a fully connected layer with softmax activation.\n",
    "\n",
    "    The ResNet architecture is defined by the number of layers and the type of residual block used.\n",
    "    The ResNet-18 and ResNet-34 architectures use the \"basic block\" residual block.\n",
    "    The ResNet-50, ResNet-101, and ResNet-152 architectures use the \"bottleneck\" residual block.\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)    # delete maxpool layer\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        \"\"\"\n",
    "        Create a layer of residual blocks\n",
    "\n",
    "        Args:\n",
    "        block: nn.Module, type of residual block: BasicBlock or Bottleneck\n",
    "        planes: int, number of output channels\n",
    "        blocks: int, number of blocks\n",
    "        \"\"\"\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet18(**kwargs):\n",
    "    return _resnet(BasicBlock, [2, 2, 2, 2],**kwargs)\n",
    "\n",
    "def ResNet34(**kwargs):\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3],**kwargs)\n",
    "\n",
    "\n",
    "def ResNet50(**kwargs):\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3],**kwargs)\n",
    "\n",
    "\n",
    "def ResNet101(**kwargs):\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3],**kwargs)\n",
    "\n",
    "\n",
    "def ResNet152(**kwargs):\n",
    "    return _resnet(Bottleneck, [3, 8, 36, 3],**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"\n",
    "    Randomly mask out one or more patches from an image.\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "        \t# (x,y) makes the center of the hole\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0 # number of subprocesses to use for data loading\n",
    "batch_size = 16 # how many samples per batch to load\n",
    "valid_size = 0.2    # percentage of training set to use as validation\n",
    "\n",
    "def read_dataset(batch_size=16,valid_size=0.2,num_workers=0,pic_path='dataset'):\n",
    "    \"\"\"\n",
    "    batch_size: Number of loaded drawings per batch\n",
    "    valid_size: Percentage of training set to use as validation\n",
    "    num_workers: Number of subprocesses to use for data loading\n",
    "    pic_path: The path of the pictrues\n",
    "    \"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),  # first, we perform a random crop of the image to 32x32 using padding of 4 pixels\n",
    "        transforms.RandomHorizontalFlip(),  # randomly flip the image horizontally, probability of 0.5\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]), # normalize the image based on the mean and standard deviation from the ImageNet dataset\n",
    "        Cutout(n_holes=1, length=16),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Define the data loaders\n",
    "    train_data = datasets.CIFAR10(pic_path, train=True,\n",
    "                                download=True, transform=transform_train)\n",
    "    valid_data = datasets.CIFAR10(pic_path, train=True,\n",
    "                                download=True, transform=transform_test)\n",
    "    test_data = datasets.CIFAR10(pic_path, train=False,\n",
    "                                download=True, transform=transform_test)\n",
    "        \n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    # random indices\n",
    "    np.random.shuffle(indices)\n",
    "    # the ratio of split\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    # divide data to radin_data and valid_data\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    # 无放回地按照给定的索引列表采样样本元素\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
    "        sampler=valid_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "        num_workers=num_workers)\n",
    "\n",
    "    return train_loader,valid_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_loader,valid_loader,test_loader = read_dataset(batch_size=batch_size,pic_path='dataset')\n",
    "n_class = 10\n",
    "model = ResNet50()\n",
    "\"\"\"\n",
    "ResNet18网络的7x7降采样卷积和池化操作容易丢失一部分信息,\n",
    "所以在实验中我们将7x7的降采样层和最大池化层去掉,替换为一个3x3的降采样卷积,\n",
    "同时减小该卷积层的步长和填充大小\n",
    "\"\"\"\n",
    "model.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "# model.fc = torch.nn.Linear(512, n_class) # modify the last layer\n",
    "model.fc = torch.nn.Linear(2048, n_class) # modify the last layer\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)    # Use cross entropy loss function\n",
    "n_epochs = 250\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/250 [00:54<3:44:49, 54.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.250791139240505 %\n",
      "Epoch: 1 \tTraining Loss: 2.981101 \tValidation Loss: 2.058987\n",
      "Validation loss decreased (inf --> 2.058987).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/250 [01:47<3:41:54, 53.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.063291139240505 %\n",
      "Epoch: 2 \tTraining Loss: 1.917678 \tValidation Loss: 1.918332\n",
      "Validation loss decreased (2.058987 --> 1.918332).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/250 [02:40<3:40:26, 53.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.68908227848101 %\n",
      "Epoch: 3 \tTraining Loss: 1.792766 \tValidation Loss: 1.656122\n",
      "Validation loss decreased (1.918332 --> 1.656122).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/250 [03:34<3:38:58, 53.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.13805379746835 %\n",
      "Epoch: 4 \tTraining Loss: 1.663207 \tValidation Loss: 1.565614\n",
      "Validation loss decreased (1.656122 --> 1.565614).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/250 [04:26<3:37:04, 53.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.74129746835443 %\n",
      "Epoch: 5 \tTraining Loss: 1.542320 \tValidation Loss: 1.631913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/250 [05:19<3:35:56, 53.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.76898734177215 %\n",
      "Epoch: 6 \tTraining Loss: 1.422609 \tValidation Loss: 1.269567\n",
      "Validation loss decreased (1.565614 --> 1.269567).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7/250 [06:12<3:34:23, 52.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.77017405063291 %\n",
      "Epoch: 7 \tTraining Loss: 1.315771 \tValidation Loss: 1.289753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/250 [07:04<3:33:00, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.94976265822785 %\n",
      "Epoch: 8 \tTraining Loss: 1.240027 \tValidation Loss: 1.119532\n",
      "Validation loss decreased (1.269567 --> 1.119532).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 9/250 [07:57<3:32:10, 52.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.38726265822785 %\n",
      "Epoch: 9 \tTraining Loss: 1.178952 \tValidation Loss: 1.297192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/250 [08:50<3:31:05, 52.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.31091772151899 %\n",
      "Epoch: 10 \tTraining Loss: 1.117056 \tValidation Loss: 1.001504\n",
      "Validation loss decreased (1.119532 --> 1.001504).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 11/250 [09:42<3:29:25, 52.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.24485759493671 %\n",
      "Epoch: 11 \tTraining Loss: 1.069930 \tValidation Loss: 1.414002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 12/250 [10:35<3:28:30, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.3073575949367 %\n",
      "Epoch: 12 \tTraining Loss: 1.028602 \tValidation Loss: 0.923823\n",
      "Validation loss decreased (1.001504 --> 0.923823).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 13/250 [11:27<3:27:04, 52.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.71875 %\n",
      "Epoch: 13 \tTraining Loss: 0.994353 \tValidation Loss: 1.139246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 14/250 [12:19<3:25:43, 52.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.48892405063291 %\n",
      "Epoch: 14 \tTraining Loss: 0.965468 \tValidation Loss: 1.057055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 15/250 [13:11<3:24:23, 52.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.2440664556962 %\n",
      "Epoch: 15 \tTraining Loss: 0.938228 \tValidation Loss: 1.112904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 16/250 [14:03<3:23:26, 52.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.69185126582279 %\n",
      "Epoch: 16 \tTraining Loss: 0.908162 \tValidation Loss: 1.004149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 17/250 [14:55<3:22:26, 52.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.86511075949367 %\n",
      "Epoch: 17 \tTraining Loss: 0.892051 \tValidation Loss: 0.806802\n",
      "Validation loss decreased (0.923823 --> 0.806802).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 18/250 [15:47<3:21:38, 52.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.61392405063292 %\n",
      "Epoch: 18 \tTraining Loss: 0.877769 \tValidation Loss: 1.068449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 19/250 [16:39<3:20:41, 52.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.93512658227848 %\n",
      "Epoch: 19 \tTraining Loss: 0.847440 \tValidation Loss: 1.112971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/250 [17:31<3:19:51, 52.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.80696202531645 %\n",
      "Epoch: 20 \tTraining Loss: 0.836597 \tValidation Loss: 0.864065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21/250 [18:23<3:18:56, 52.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.14200949367088 %\n",
      "Epoch: 21 \tTraining Loss: 0.815967 \tValidation Loss: 0.802508\n",
      "Validation loss decreased (0.806802 --> 0.802508).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 22/250 [19:15<3:17:54, 52.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.29153481012658 %\n",
      "Epoch: 22 \tTraining Loss: 0.801075 \tValidation Loss: 0.781549\n",
      "Validation loss decreased (0.802508 --> 0.781549).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 23/250 [20:07<3:16:58, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.44224683544304 %\n",
      "Epoch: 23 \tTraining Loss: 0.780723 \tValidation Loss: 0.891987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 24/250 [20:59<3:15:56, 52.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.32357594936708 %\n",
      "Epoch: 24 \tTraining Loss: 0.762734 \tValidation Loss: 0.866097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 25/250 [21:51<3:15:05, 52.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.61669303797468 %\n",
      "Epoch: 25 \tTraining Loss: 0.758850 \tValidation Loss: 0.801038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 26/250 [22:43<3:14:18, 52.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.50791139240506 %\n",
      "Epoch: 26 \tTraining Loss: 0.744142 \tValidation Loss: 0.858619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 27/250 [23:36<3:13:30, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.49802215189874 %\n",
      "Epoch: 27 \tTraining Loss: 0.729185 \tValidation Loss: 0.831705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 28/250 [24:27<3:12:30, 52.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.72191455696202 %\n",
      "Epoch: 28 \tTraining Loss: 0.717790 \tValidation Loss: 0.698764\n",
      "Validation loss decreased (0.781549 --> 0.698764).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 29/250 [25:19<3:11:25, 51.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.51503164556962 %\n",
      "Epoch: 29 \tTraining Loss: 0.708904 \tValidation Loss: 1.085134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/250 [26:11<3:10:38, 51.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.38053797468355 %\n",
      "Epoch: 30 \tTraining Loss: 0.699051 \tValidation Loss: 0.889145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 31/250 [27:03<3:09:51, 52.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.78125 %\n",
      "Epoch: 31 \tTraining Loss: 0.688669 \tValidation Loss: 0.742737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 32/250 [27:56<3:09:12, 52.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.06803797468355 %\n",
      "Epoch: 32 \tTraining Loss: 0.691144 \tValidation Loss: 0.696963\n",
      "Validation loss decreased (0.698764 --> 0.696963).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 33/250 [28:48<3:08:24, 52.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.82950949367088 %\n",
      "Epoch: 33 \tTraining Loss: 0.682039 \tValidation Loss: 0.662504\n",
      "Validation loss decreased (0.696963 --> 0.662504).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 34/250 [29:40<3:07:30, 52.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.03955696202532 %\n",
      "Epoch: 34 \tTraining Loss: 0.665076 \tValidation Loss: 0.726776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 35/250 [30:32<3:06:30, 52.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.75870253164557 %\n",
      "Epoch: 35 \tTraining Loss: 0.661217 \tValidation Loss: 1.039470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 36/250 [31:24<3:05:37, 52.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.0557753164557 %\n",
      "Epoch: 36 \tTraining Loss: 0.659412 \tValidation Loss: 0.633696\n",
      "Validation loss decreased (0.662504 --> 0.633696).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 37/250 [32:16<3:04:49, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.5996835443038 %\n",
      "Epoch: 37 \tTraining Loss: 0.653842 \tValidation Loss: 0.618147\n",
      "Validation loss decreased (0.633696 --> 0.618147).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 38/250 [33:10<3:05:55, 52.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.82713607594937 %\n",
      "Epoch: 38 \tTraining Loss: 0.646645 \tValidation Loss: 0.609647\n",
      "Validation loss decreased (0.618147 --> 0.609647).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 39/250 [34:02<3:04:20, 52.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.98378164556962 %\n",
      "Epoch: 39 \tTraining Loss: 0.638124 \tValidation Loss: 0.923956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 40/250 [34:54<3:03:08, 52.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.18196202531645 %\n",
      "Epoch: 40 \tTraining Loss: 0.638057 \tValidation Loss: 0.563541\n",
      "Validation loss decreased (0.609647 --> 0.563541).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 41/250 [35:46<3:02:10, 52.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.01107594936708 %\n",
      "Epoch: 41 \tTraining Loss: 0.637937 \tValidation Loss: 0.850016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 42/250 [36:38<3:01:18, 52.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.54746835443038 %\n",
      "Epoch: 42 \tTraining Loss: 0.628437 \tValidation Loss: 0.964258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 43/250 [37:30<3:00:01, 52.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.95648734177215 %\n",
      "Epoch: 43 \tTraining Loss: 0.625124 \tValidation Loss: 0.991361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 44/250 [38:22<2:59:04, 52.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.0245253164557 %\n",
      "Epoch: 44 \tTraining Loss: 0.619876 \tValidation Loss: 0.980906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 45/250 [39:15<2:58:19, 52.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.27412974683544 %\n",
      "Epoch: 45 \tTraining Loss: 0.621057 \tValidation Loss: 1.009751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 46/250 [40:07<2:57:22, 52.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.66772151898734 %\n",
      "Epoch: 46 \tTraining Loss: 0.608087 \tValidation Loss: 0.576931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 47/250 [40:59<2:56:24, 52.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.04905063291139 %\n",
      "Epoch: 47 \tTraining Loss: 0.612677 \tValidation Loss: 1.106255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 48/250 [41:51<2:55:31, 52.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.80617088607595 %\n",
      "Epoch: 48 \tTraining Loss: 0.604034 \tValidation Loss: 0.602352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 49/250 [42:43<2:54:48, 52.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.98615506329114 %\n",
      "Epoch: 49 \tTraining Loss: 0.605364 \tValidation Loss: 0.923430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 49/250 [43:36<2:58:51, 53.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.00356012658227 %\n",
      "Epoch: 50 \tTraining Loss: 0.587802 \tValidation Loss: 0.916616\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stop_threshold = 10\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "accuracy = []\n",
    "\n",
    "counter = 0\n",
    "for epoch in tqdm(range(1, n_epochs+1)):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total_sample = 0\n",
    "    right_sample = 0\n",
    "    \n",
    "    # Dynamic learning rate\n",
    "    if counter/10 ==1:\n",
    "        counter = 0\n",
    "        lr = lr*0.5\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    ###################\n",
    "    # 训练集的模型 #\n",
    "    ###################\n",
    "    model.train() #作用是启用batch normalization和drop out\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # clear the gradients of all optimized variables（清除梯度）\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data).to(device)  #（等价于output = model.forward(data).to(device) ）\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss（\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # 验证集的模型#\n",
    "    ######################\n",
    "\n",
    "    model.eval()  # 验证模型\n",
    "    for data, target in valid_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data).to(device)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class(将输出概率转换为预测类)\n",
    "        _, pred = torch.max(output, 1)    \n",
    "        # compare predictions to true label(将预测与真实标签进行比较)\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        # correct = np.squeeze(correct_tensor.to(device).numpy())\n",
    "        total_sample += batch_size\n",
    "        for i in correct_tensor:\n",
    "            if i:\n",
    "                right_sample += 1\n",
    "    print(\"Accuracy:\",100*right_sample/total_sample,\"%\")\n",
    "    accuracy.append(right_sample/total_sample)\n",
    " \n",
    "    # 计算平均损失\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # 显示训练集与验证集的损失函数 \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # 如果验证集损失函数减少，就保存模型。\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        torch.save(model.state_dict(), 'checkpoint/resnet50_cifar10.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= early_stop_threshold:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_class = 10\n",
    "batch_size = 100\n",
    "train_loader,valid_loader,test_loader = read_dataset(batch_size=batch_size,pic_path='dataset')\n",
    "model = ResNet50() # 得到预训练模型\n",
    "model.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "# model.fc = torch.nn.Linear(512, n_class) # 将最后的全连接层修改\n",
    "model.fc = torch.nn.Linear(2048, n_class) # modify the last layer\n",
    "\n",
    "# 载入权重\n",
    "model.load_state_dict(torch.load('checkpoint/resnet50_cifar10.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Accuracy: 95.14 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_sample = 0\n",
    "right_sample = 0\n",
    "model.eval()  # 验证模型\n",
    "for data, target in test_loader:\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data).to(device)\n",
    "    # convert output probabilities to predicted class(将输出概率转换为预测类)\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label(将预测与真实标签进行比较)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.to(device).numpy())\n",
    "    total_sample += batch_size\n",
    "    for i in correct_tensor:\n",
    "        if i:\n",
    "            right_sample += 1\n",
    "print(\"Accuracy:\",100*right_sample/total_sample,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
